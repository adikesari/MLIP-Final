name: sr4ir_deblur
model_type: sr4ir_deblur
num_threads: 16
print_freq: 100  # unit: iter
manual_seed: 100
scale: 8
deterministic: true
#test_only: true

# data and augmentation
data:
  path: datasets/deblur
  format: deblur
  aspect_ratio_group_factor: 3
  data_augmentation: hflip
  is_voc: false
  mean: [0.4488, 0.4371, 0.4040]
  std: [1.0, 1.0, 1.0]
  train:
    name: deblur_train
    type: DeblurDataset
    batch_size: 4
    num_workers: 4
    use_shuffle: true
    data_root: ./datasets/deblur/train
  val:
    name: deblur_val
    type: DeblurDataset
    batch_size: 1
    num_workers: 4
    use_shuffle: false
    data_root: ./datasets/deblur/val

# dataset configurations
datasets:
  train:
    name: deblur_train
    type: DeblurDataset
    batch_size: 4
    num_workers: 4
    use_shuffle: true
    data_root: ./datasets/deblur/train
    mean: [0.4488, 0.4371, 0.4040]
    std: [1.0, 1.0, 1.0]

  val:
    name: deblur_val
    type: DeblurDataset
    batch_size: 1
    num_workers: 4
    use_shuffle: false
    data_root: ./datasets/deblur/val
    mean: [0.4488, 0.4371, 0.4040]
    std: [1.0, 1.0, 1.0]

# network specs
network_sr:
  name: edsr
  n_blocks: 16
  n_feats: 64


network_deblur:
  name: nafnet
  img_channel: 9  # 3 channels each for SR, HR, and CQMix
  width: 16
  middle_blk_num: 6
  enc_blk_nums: [1, 1, 2, 4]
  dec_blk_nums: [1, 1, 1, 1]
  return_feat: true

# path for pretrained model
path:
  network_sr: experiments/pretrained_models/edsr_baseline_x8.pt
  network_deblur: null

# training config
train:  
  batch_size: 4  # 1 GPU
  epoch: 30
  save_freq: 10  # unit: epoch
  eval_freq: 10
  warmup_epoch: 3

  # optimizer
  optim_sr:
    type: Adam
    lr: !!float 1e-4
    weight_decay: 0
    betas: [0.9, 0.999]
  optim_deblur:
    type: Adam
    lr: !!float 1e-4
    weight_decay: 0
    betas: [0.9, 0.999]
  
  # scheduler
  scheduler_sr:
    type: CosineAnnealingRestartLR
    periods: [2, 27]  # 1 for warmup
    restart_weights: [1, 1]
    eta_min: !!float 1e-6
  scheduler_deblur:
    type: CosineAnnealingRestartLR
    periods: [2, 27]  # 1 for warmup
    restart_weights: [1, 1]
    eta_min: !!float 1e-6

  # phase 1 losses
  pixel_opt:
    type: MSELoss
    loss_weight: !!float 1.0
    reduction: mean
  tdp_opt:
    type: L1Loss
    loss_weight: !!float 1.0
    reduction: mean
  
  # phase 2 losses
  deblur_sr_opt:
    type: MSELoss
    loss_weight: !!float 0.34
  deblur_hr_opt:
    type: MSELoss
    loss_weight: !!float 0.34
  deblur_cqmix_opt:
    type: MSELoss
    loss_weight: !!float 0.34

# training config
test:  
  batch_size: 1
  visualize: true
  calculate_lpips: true

# DDP setting
dist_url: env://
dist: false
world_size: 1
rank: 0
find_unused_parameters: false
broadcast_buffers: false

# Device configuration
device: cuda
distributed: false
gpu_ids: [0]  # Use first GPU, modify this list to use multiple GPUs

# Logging configurations
logger:
  print_freq: 100
  save_checkpoint_freq: 5000
  use_tb_logger: true
  tb_logger:
    log_dir: ./tb_logger/sr4ir_deblur 